<!-- Publications page -->

<html>
<head>
    <title>Publications - Vardhan Dongre</title>
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="stylesheet" type="text/css" href="css/publications.css">
</head>

<body>
    <div class="content">
        <nav class="navigation">
            <a href="index.html">← Back to Home</a>
        </nav>
        
        <h1>Publications</h1>
        <h2>Research Papers & Publications</h2>
        
        <div class="publications-container">
            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations</h3>
                    <span class="publication-year">2025</span>
                </div>
                <div class="publication-authors">
                    <strong>Vardhan Dongre</strong>, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-Tür, Vikram S. Adve
                </div>
                <div class="publication-venue">
                    arXiv preprint
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2506.20100" class="publication-link" target="_blank">Paper</a>
                    <a href="https://github.com/MIRAGE-Benchmark/MIRAGE-Benchmark" class="publication-link" target="_blank">Code</a>
                    <a href="https://mirage-benchmark.github.io/" class="publication-link" target="_blank">Leaderboard</a>
                </div>
                <!-- <div class="publication-abstract">
                    We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning and decision-making in consultative interaction settings. Designed for the agriculture domain, MIRAGE captures the full complexity of expert consultations by combining natural user queries, expert-authored responses, and image-based context, offering a high-fidelity benchmark for evaluating models on grounded reasoning, clarification strategies, and long-form generation in a real-world, knowledge-intensive domain. Grounded in over 35,000 real user-expert interactions and curated through a carefully designed multi-step pipeline, MIRAGE spans diverse crop health, pest diagnosis, and crop management scenarios. The benchmark includes more than 7,000 unique biological entities, covering plant species, pests, and diseases, making it one of the most taxonomically diverse benchmarks available for vision-language models, grounded in the real world.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions</h3>
                    <span class="publication-year">2025</span>
                </div>
                <div class="publication-authors">
                    Emre Can Acikgoz, Cheng Qian, Hongru Wang, <strong>Vardhan Dongre</strong>, Xiusi Chen, Heng Ji, Dilek Hakkani-Tür, Gokhan Tur
                </div>
                <div class="publication-venue">
                    arXiv preprint
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2504.16939" class="publication-link" target="_blank">Paper</a>
                </div>
                <!-- <div class="publication-abstract">
                    Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. We systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">Better Slow than Sorry: Introducing Positive Friction for Reliable Dialogue Systems</h3>
                    <span class="publication-year">2025</span>
                </div>
                <div class="publication-authors">
                    Mert İnan, Anthony Sicilia, Suvodip Dey, <strong>Vardhan Dongre</strong>, Tejas Srinivasan, Jesse Thomason, Gökhan Tür, Dilek Hakkani-Tür, Malihe Alikhani
                </div>
                <div class="publication-venue">
                    Transactions of the Association for Computational Linguistics
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2501.17348" class="publication-link" target="_blank">Paper</a>
                    <a href="https://github.com/Merterm/Positive-Friction-Dialogue" class="publication-link" target="_blank">Code</a>
                </div>
                <!-- <div class="publication-abstract">
                    While theories of discourse and cognitive science have long recognized the value of unhurried pacing, recent dialogue research tends to minimize friction in conversational systems. Yet, frictionless dialogue risks fostering uncritical reliance on AI outputs, which can obscure implicit assumptions and lead to unintended consequences. To meet this challenge, we propose integrating positive friction into conversational AI, which promotes user reflection on goals, critical thinking on system response, and subsequent re-conditioning of AI systems. We hypothesize systems can improve goal alignment, modeling of user mental states, and task success by deliberately slowing down conversations in strategic moments to ask questions, reveal assumptions, or pause.
                </div> -->
            </div>

            <div class="publication">
                <div class="publication-header">
                    <h3 class="publication-title">ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</h3>
                    <span class="publication-year">2024</span>
                </div>
                <div class="publication-authors">
                    <strong>Vardhan Dongre</strong>, Xiaocheng Yang, Emre Can Acikgoz, Suvodip Dey, Gokhan Tur, Dilek Hakkani-Tür
                </div>
                <div class="publication-venue">
                    International Workshop on Spoken Dialogue Systems Technology (Oral)
                </div>
                <div class="publication-links">
                    <a href="https://arxiv.org/abs/2411.00927" class="publication-link" target="_blank">Paper</a>
                    <a href="https://github.com/vardhandongre/Respact" class="publication-link" target="_blank">Code</a>
                </div>
                <!-- <div class="publication-abstract">
                    Large language model (LLM)-based agents are increasingly employed to interact with external environments to solve user-provided tasks. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. ReSpAct demonstrates improved performance across diverse environments including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop).
                </div> -->
            </div>
        </div>

        <div class="publications-note">
            <p>
                For a complete list of publications, please visit my 
                <a href="https://scholar.google.com/citations?user=sSt2OvIAAAAJ&hl=en&authuser=1">Google Scholar profile</a>.
            </p>
        </div>
    </div>
</body>
</html> 